{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFiu8HLsLi1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pwN_jAasVhA",
        "outputId": "32fe7bc0-bc15-4950-d7ce-982cbe67a033"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlmJmbSOsbPJ"
      },
      "source": [
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ_5oTD_HG5d"
      },
      "source": [
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, classes):\n",
        "    # count all samples at split point\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    # sum weighted Gini index for each group\n",
        "    gini = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        # score the group based on the score for each class\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            score += p * p\n",
        "        # weight the group score by its relative size\n",
        "        gini += (1.0 - score) * (size / n_instances)\n",
        "    return gini\n",
        "# Select the best split point for a dataset\n",
        "def get_split_gini(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    gini_all=[]\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        value= np.mean([row[index] for row in dataset])\n",
        "        groups = test_split(index, value, dataset)\n",
        "        gini = gini_index(groups, class_values)\n",
        "        gini_all.append([gini,index,value,groups])\n",
        "    [s_gini,s_index,s_value,s_groups]= min(gini_all,key= lambda x: x[0])\n",
        "    return {'index':s_index, 'value':s_value, 'groups':s_groups}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEaOBmH3JHoH"
      },
      "source": [
        "def info_gain(groups, classes,dataset):\n",
        "    # count all samples at split point\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    infogain = 0.0\n",
        "    for class_val in classes:\n",
        "            p = [row[-1] for row in dataset].count(class_val) / n_instances\n",
        "            if p == 0.0 :\n",
        "              continue\n",
        "            infogain += - p * np.log2(p)\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        # score the group based on the score for each class\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            if p == 0.0 :\n",
        "              continue\n",
        "            score += - p * np.log2(p)\n",
        "        # weight the group score by its relative size\n",
        "        infogain -= (score) * (size / n_instances)\n",
        "    return infogain\n",
        "# Select the best split point for a dataset\n",
        "def get_split_info_gain(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    info_all=[]\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        value= np.mean([row[index] for row in dataset])\n",
        "        groups = test_split(index, value, dataset)\n",
        "        info = info_gain(groups, class_values,dataset)\n",
        "        info_all.append([info,index,value,groups])\n",
        "    [s_info,s_index,s_value,s_groups]= max(info_all,key= lambda x: x[0])\n",
        "    return {'index':s_index, 'value':s_value, 'groups':s_groups}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ctXqvGJ7dR"
      },
      "source": [
        "def gain_ratio(groups, classes,dataset):\n",
        "    # count all samples at split point\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    infogain = 0.0\n",
        "    splitinfo = 0.0\n",
        "    for class_val in classes:\n",
        "            p = [row[-1] for row in dataset].count(class_val) / n_instances\n",
        "            if p == 0.0 :\n",
        "              continue\n",
        "            infogain += - p * np.log2(p)\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        n=size/n_instances\n",
        "        splitinfo -=  n * np.log2(n)\n",
        "        score = 0.0\n",
        "        # score the group based on the score for each class\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            if p == 0.0 :\n",
        "              continue\n",
        "            score += - p * np.log2(p)\n",
        "        # weight the group score by its relative size\n",
        "        infogain -= (score) * (size / n_instances)\n",
        "    # print(splitinfo)\n",
        "    if splitinfo != 0.0 :\n",
        "      return infogain/splitinfo\n",
        "    else :\n",
        "      return None\n",
        "# Select the best split point for a dataset\n",
        "def get_split_gain_ratio(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    gr_all=[]\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        value=np.mean([row[index] for row in dataset])\n",
        "        groups = test_split(index, value, dataset)\n",
        "        gr = gain_ratio(groups, class_values,dataset)\n",
        "        if gr != None:\n",
        "          gr_all.append([gr,index,value,groups])\n",
        "    [s_gr,s_index,s_value,s_groups]= max(gr_all,key= lambda x: x[0])\n",
        "    return {'index':s_index, 'value':s_value, 'groups':s_groups}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3DLK3GtMYBC"
      },
      "source": [
        "def mc_error(groups, classes):\n",
        "    # count all samples at split point\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    error = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        prob = []\n",
        "        # score the group based on the score for each class\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            prob.append(p)\n",
        "        # weight the group score by its relative size\n",
        "        \n",
        "        error += (1.0 - np.amax(prob)) * (size / n_instances)\n",
        "    return error\n",
        "# Select the best split point for a dataset\n",
        "def get_split_mc_error(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    error_all=[]\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        value=np.mean([row[index] for row in dataset])\n",
        "        groups = test_split(index, value, dataset)\n",
        "        error = mc_error(groups, class_values)\n",
        "        error_all.append([error,index,value,groups])\n",
        "    [s_error,s_index,s_value,s_groups]= min(error_all,key= lambda x: x[0])\n",
        "    return {'index':s_index, 'value':s_value, 'groups':s_groups}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRBTtVYCNAks"
      },
      "source": [
        "def chi_square(groups, classes,dataset):\n",
        "    # count all samples at split point\n",
        "    count_group=[len(group) for group in groups]\n",
        "    n_instances = float(sum(count_group))\n",
        "    count_class=[]\n",
        "    for class_val in classes:\n",
        "            count_class.append([row[-1] for row in dataset].count(class_val))\n",
        "    expected_matrix=[c*g/n_instances for g in count_group for c in count_class]\n",
        "    chi = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        # avoid divide by zero\n",
        "        if size == 0:\n",
        "            continue\n",
        "        prob = []\n",
        "        for class_val in classes:\n",
        "            n = [row[-1] for row in group].count(class_val) \n",
        "            ind=(groups.index(group)*2 + classes.index(class_val))\n",
        "            chi+= (n-expected_matrix[ind])**2/expected_matrix[ind]\n",
        "    return chi\n",
        "# Select the best split point for a dataset\n",
        "def get_split_chi_square(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    chi_all=[]\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        value=np.mean([row[index] for row in dataset])\n",
        "        groups = test_split(index, value, dataset)\n",
        "        chi = chi_square(groups, class_values,dataset)\n",
        "        chi_all.append([chi,index,value,groups])\n",
        "    [s_chi,s_index,s_value,s_groups]= max(chi_all,key= lambda x: x[0])\n",
        "    return {'index':s_index, 'value':s_value, 'groups':s_groups}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt9EN4jVNmBZ"
      },
      "source": [
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve5bl7_ar2xy"
      },
      "source": [
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_leafsize, depth,get_split_type): \n",
        "  left, right = node['groups'] \n",
        "  del(node['groups'])\n",
        "  # check for a no split\n",
        "  if not left or not right:\n",
        "      node['left'] = node['right'] = to_terminal(left + right)\n",
        "      return\n",
        "  # check for max depth\n",
        "  if depth >= max_depth:\n",
        "      node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "      return\n",
        "  # process left child\n",
        "  if len(left) <= min_leafsize or len(set([row[-1] for row in left]))==1:\n",
        "      node['left'] = to_terminal(left)\n",
        "  else:\n",
        "      node['left'] = get_split_type(left)\n",
        "      split(node['left'], max_depth, min_leafsize, depth+1,get_split_type)\n",
        "  # process right child\n",
        "  if len(right) <= min_leafsize or len(set([row[-1] for row in right]))==1:\n",
        "      node['right'] = to_terminal(right)\n",
        "  else:\n",
        "      node['right'] = get_split_type(right)\n",
        "      split(node['right'], max_depth, min_leafsize, depth+1,get_split_type)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6ZuxYCxRtSc"
      },
      "source": [
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_leafsize,get_split_type):\n",
        "    root = get_split_type(train)\n",
        "    split(root, max_depth, min_leafsize, 1,get_split_type)\n",
        "    return root\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiUN_XMNcdqZ"
      },
      "source": [
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrRsNCMNcfiV"
      },
      "source": [
        "def decision_tree(train, test, max_depth, min_leafsize,get_split_type):\n",
        "    tree = build_tree(train, max_depth, min_leafsize,get_split_type)\n",
        "    predictions =[predict(tree, row) for row in test]\n",
        "    return predictions"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCcZggmHfOqB"
      },
      "source": [
        "def evaluate_DT_withKfold(dataset, n_folds, max_depth, min_leafsize,criterion):\n",
        "  criteria={'gini': get_split_gini,'infogain': get_split_info_gain,'gainratio': get_split_gain_ratio,\n",
        "             'mc_error': get_split_mc_error,'chi_square': get_split_chi_square}\n",
        "  kf = KFold(n_splits=n_folds,shuffle=True,random_state=0)\n",
        "  actual=[]\n",
        "  pred=[]\n",
        "  # actual=[row[-1] for row in test]\n",
        "  for train_index, test_index in kf.split(dataset):\n",
        "      train=(np.array(dataset)[train_index,:]).tolist()\n",
        "      test=(np.array(dataset)[test_index,:]).tolist()\n",
        "      actual.extend([row[-1] for row in test])\n",
        "      for row in test:\n",
        "        row[-1] =None\n",
        "      predictions=decision_tree(train, test, max_depth, min_leafsize,criteria[criterion])\n",
        "      pred.extend(predictions)\n",
        "  return accuracy_score(actual,pred),f1_score(actual,pred)\n",
        "\n",
        "      "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfqJuq-zoUnY",
        "outputId": "1f6fd788-bddc-4fa0-9979-62295a5a7873"
      },
      "source": [
        "fval=np.zeros((56,5))\n",
        "acv=np.zeros((56,5))\n",
        "criteria=['gini','infogain','gainratio','mc_error','chi_square']\n",
        "n_folds=10\n",
        "max_depth=[6,6,6,6,6]\n",
        "min_leafsize=[5,5,5,5,5]\n",
        "with tf.device('/device:GPU:0'):\n",
        "  for i in range(1,57):#[12,15,21]:\n",
        "    print(i)\n",
        "    df=pd.read_csv('/content/drive/My Drive/DM_assignment/'+str(i)+'.csv',header= None)\n",
        "    dfnew=df.copy()\n",
        "    dfnew[len(df.columns)-1]=df[len(df.columns)-1].map(lambda x : int(x>0))\n",
        "    dflist=dfnew.values.tolist()\n",
        "    for j in range(0,5):\n",
        "      acv[i-1,j],fval[i-1,j]=evaluate_DT_withKfold(dflist, n_folds, max_depth[j], min_leafsize[j],criteria[j])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGyUSC4rzAvb"
      },
      "source": [
        "df_acv=pd.DataFrame(acv,index=range(1,57),columns=criteria)\n",
        "df_acv.to_csv('/content/drive/My Drive/DM_assignment2/accuracy.csv')\n",
        "\n",
        "df_fval=pd.DataFrame(fval,index=range(1,57),columns=criteria)\n",
        "df_fval.to_csv('/content/drive/My Drive/DM_assignment2/Fmeasure.csv')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pFbzSZc2WiU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}